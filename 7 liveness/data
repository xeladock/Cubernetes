The first Pod, bad-frontend is an HTTP service which always returns a 500 error indicating it hasn't started correctly. You can view the status of the Pod with kubectl get pods --selector="name=bad-frontend"

Kubectl will return the Pods deployed with our particular label. Because the healthcheck is failing, it will say that zero containers are ready. It will also indicate the number of restart attempts of the container.

To find out more details of why it's failing, describe the Pod.

pod=$(kubectl get pods --selector="name=bad-frontend" --output=jsonpath={.items..metadata.name})
kubectl describe pod $pod

Readiness OK
Our second Pod, frontend, returns an OK status on launch.

kubectl get pods --selector="name=frontend"

With our second Pod currently running in a health state, we can simulate a failure occurring.

At present, no crashes should have occurred. kubectl get pods --selector="name=frontend"

Crash Service
The HTTP server has an additional endpoint that will cause it to return 500 errors. Using kubectl exec it's possible to call the endpoint.

pod=$(kubectl get pods --selector="name=frontend" --output=jsonpath={.items..metadata.name})
kubectl exec $pod -- /usr/bin/curl -s localhost/unhealthy

Liveness
Based on the configuration, Kubernetes will execute the Liveness Probe. If the Probe fails, Kubernetes will destroy and re-create the failed container. Execute the above command to crash the service and watch Kubernetes recover it automatically.

kubectl get pods --selector="name=frontend"

The check may take a few moments to detect.
